{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Regression Tutorial\n","\n","Regression is a fundamental technique in Machine Learning used to model relationships between variables. It is widely applied to predict outcomes based on input data.\n","\n","Regression can be broadly categorized into:  \n","1. **Linear Regression**: Assumes a linear relationship between the variables.  \n","2. **Polynomial Regression**: Captures non-linear relationships by introducing polynomial terms.\n","\n","---\n","\n","## 1. Linear Regression\n","\n","Linear regression assumes a **straight-line relationship** between the independent variable(s) and the target variable.\n","\n","---\n","\n","### Types of Linear Regression  \n","\n","#### **Simple Linear Regression**  \n","A single independent variable is used to predict the target variable.  \n","\n","The model predicts the output \\(Y\\) as a function of the independent variable \\(X\\):  \n","\n","$$\n","Y = a + bX\n","$$\n","\n","- **$Y$**: Predictor Variable (Dependent Variable)  \n","- **$X$**: Independent Variable  \n","- **$a$**: Intercept of the regression line (value of $Y$ when $X = 0$)  \n","- **$b$**: Slope of the regression line (rate of change of $Y$ when $X$ is incremented by 1 unit).  \n","\n","- **Practical Example**:  \n","  A company uses **advertising budget** as the independent variable to predict **sales revenue**.  \n","  - **Independent Variable**: $X$ (Advertising Budget)  \n","  - **Dependent Variable**: $Y$ (Sales Revenue)  \n","  - **Goal**: Find how increasing the advertising budget impacts sales.\n","\n","<img src=\"https://drive.google.com/uc?id=1TyiL4aN66v-I3JkvJS2ZC7VFX_jy7CWy\" alt=\"Regression Example\" width=\"300\"/>\n","---\n","\n","#### **Multiple Linear Regression**  \n","Two or more independent variables are used to predict the target variable.  \n","\n","The equation for Multiple Linear Regression is:  \n","\n","$$\n","Y = a + b_1X_1 + b_2X_2 + b_3X_3 + \\dots\n","$$\n","\n","- **Coefficients**:  \n","  $$\n","  (b_1, b_2, b_3, \\dots)\n","  $$  \n","- **Independent Variables**:  \n","  $$\n","  (X_1, X_2, X_3, \\dots)\n","  $$  \n","\n","- **Practical Example**:  \n","  A real estate agency uses **property features** to predict the **house price**.  \n","  - **Independent Variables**: $(X_1, X_2, X_3)$ (Square footage, number of bedrooms, and location rating)  \n","  - **Dependent Variable**: $Y$ (House Price)  \n","  - **Goal**: Analyze how multiple factors affect housing prices.\n","\n","---\n","\n","## 2. Polynomial Regression\n","\n","Polynomial regression extends linear regression to model **non-linear relationships** between variables. It transforms the features into polynomial terms (e.g., $X^2, X^3$).\n","\n","The equation for Polynomial Regression is:  \n","\n","$$\n","Y = a + b_1X + b_2X^2 + b_3X^3 + \\dots + b_nX^n\n","$$\n","\n","- **$Y$**: Dependent Variable  \n","- **$X, X^2, X^3, \\dots$**: Polynomial terms  \n","- **Coefficients**:  \n","  $$\n","  (a, b_1, b_2, b_3, \\dots)\n","  $$  \n","\n","- **Practical Example**:  \n","   Predicting the **growth of a plant** based on **time**.  \n","   Initially, the plant grows slowly, then faster, and later the growth levels off, forming a curve.  \n","   - **Independent Variable**: $X$ (Time in days)  \n","   - **Dependent Variable**: $Y$ (Plant Height in cm)  \n","   - **Goal**: Use a polynomial regression model to predict the non-linear growth pattern over time.\n","\n","---\n","\n","## Comparison of Linear and Polynomial Regression\n","\n","| **Aspect**                 | **Linear Regression**                                | **Polynomial Regression**                     |\n","|----------------------------|-----------------------------------------------------|---------------------------------------------|\n","| **Nature of Relationship** | Assumes a straight-line relationship                | Models non-linear relationships (curved)     |\n","| **Complexity**             | Simple and interpretable                            | More complex due to polynomial terms         |\n","| **Use Cases**              | When data follows a linear trend                    | When data shows curves or non-linear trends  |\n","\n","---\n","\n","- **Linear Regression** is suitable for data with a linear relationship, like predicting sales based on advertising.  \n","- **Polynomial Regression** is ideal when the relationship is non-linear, such as growth patterns over time.  \n","\n","Understanding the type of relationship in the data helps choose the correct regression technique to make accurate predictions. ðŸš€\n"],"metadata":{"id":"cz7HrpTQHrwy"}},{"cell_type":"markdown","source":["# **Derivation of the Simple Linear Regression Formula**\n","\n","Simple Linear Regression (SLR) aims to model the relationship between one independent variable \\(X\\) and one dependent variable \\(Y\\). The relationship is represented as:\n","\n","$$\n","Y = a + bX\n","$$\n","\n","Where:  \n","- \\(Y\\): Dependent variable (target)  \n","- \\(X\\): Independent variable (predictor)  \n","- \\(a\\): Intercept (value of \\(Y\\) when \\(X = 0\\))  \n","- \\(b\\): Slope (rate of change of \\(Y\\) with respect to \\(X\\))  \n","\n","The goal is to determine the values of the slope \\(b\\) and intercept \\(a\\) that minimize the sum of squared residuals (errors). Letâ€™s derive the formula step by step.\n","\n","---\n","\n","## **1. Sum of Squared Residuals**\n","\n","The residual (error) for each data point is the difference between the observed value \\(Y_i\\) and the predicted value \\(\\hat{Y}_i\\):\n","\n","$$\n","e_i = Y_i - \\hat{Y}_i\n","$$\n","\n","The predicted value \\(\\hat{Y}_i\\) is given as:\n","\n","$$\n","\\hat{Y}_i = a + bX_i\n","$$\n","\n","Thus, the residual becomes:\n","\n","$$\n","e_i = Y_i - (a + bX_i)\n","$$\n","\n","The objective is to minimize the **Sum of Squared Errors (SSE)**:\n","\n","$$\n","SSE = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n \\left(Y_i - (a + bX_i)\\right)^2\n","$$\n","\n","---\n","\n","## **2. Deriving the Slope (\\(b\\))**\n","\n","To minimize SSE, we differentiate the SSE with respect to \\(a\\) and \\(b\\), and set the derivatives to zero.\n","\n","### **Step 1: Partial Derivative with Respect to \\(a\\)**\n","\n","Taking the derivative of SSE with respect to \\(a\\):\n","\n","$$\n","\\frac{\\partial SSE}{\\partial a} = -2 \\sum_{i=1}^n \\left(Y_i - a - bX_i\\right)\n","$$\n","\n","Set this derivative to zero:\n","\n","$$\n","\\sum_{i=1}^n (Y_i - a - bX_i) = 0\n","$$\n","\n","Rearranging, we get:\n","\n","$$\n","\\sum Y_i = na + b \\sum X_i\n","$$\n","\n","---\n","\n","### **Step 2: Partial Derivative with Respect to \\(b\\)**\n","\n","Taking the derivative of SSE with respect to \\(b\\):\n","\n","$$\n","\\frac{\\partial SSE}{\\partial b} = -2 \\sum_{i=1}^n X_i \\left(Y_i - a - bX_i\\right)\n","$$\n","\n","Set this derivative to zero:\n","\n","$$\n","\\sum_{i=1}^n X_i (Y_i - a - bX_i) = 0\n","$$\n","\n","Expanding and rearranging:\n","\n","$$\n","\\sum X_iY_i = a \\sum X_i + b \\sum X_i^2\n","$$\n","\n","---\n","\n","## **3. Solving for \\(a\\) and \\(b\\)**\n","\n","We now have two equations:\n","\n","1. $ \\sum Y_i = na + b \\sum X_i $  \n","2. $ \\sum X_iY_i = a \\sum X_i + b \\sum X_i^2 $  \n","\n","### Solving for \\(b\\) (Slope)\n","\n","First, substitute \\(a\\) from the first equation into the second equation.\n","\n","After simplification, the slope \\(b\\) is given as:\n","\n","$$\n","b = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2}\n","$$\n","\n","Where:  \n","- $\\bar{X}$: Mean of $X$ values  \n","- $\\bar{Y}$: Mean of $Y$ values  \n","\n","The numerator represents the **covariance** between \\(X\\) and \\(Y\\), and the denominator represents the **variance** of \\(X\\).\n","\n","---\n","\n","### Solving for \\(a\\) (Intercept)\n","\n","Once we have \\(b\\), the intercept \\(a\\) can be calculated using:\n","\n","$$\n","a = \\bar{Y} - b\\bar{X}\n","$$\n","\n","---\n","\n","## **4. Final SLR Formula**\n","\n","The final Simple Linear Regression equation is:\n","\n","$$\n","Y = a + bX\n","$$\n","\n","Where:  \n","- \\(b\\) (Slope):  \n","\n","$$\n","b = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2}\n","$$  \n","\n","- \\(a\\) (Intercept):  \n","\n","$$\n","a = \\bar{Y} - b\\bar{X}\n","$$\n","\n","---\n","\n","## **Summary**\n","\n","1. **Slope \\(b\\)** measures the rate of change of \\(Y\\) with respect to \\(X\\) (calculated as covariance of \\(X\\) and \\(Y\\) divided by variance of \\(X\\)).  \n","2. **Intercept \\(a\\)** determines the value of \\(Y\\) when \\(X = 0\\).  \n","3. Together, \\(a\\) and \\(b\\) define the best-fit line that minimizes the sum of squared residuals.\n","\n","The derived formulas allow us to find the optimal regression line for any given set of data. ðŸš€\n","\n","\n"],"metadata":{"id":"zud2WgQoR5M1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-yeaxiLlvxcM"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.datasets import fetch_california_housing\n","import pandas as pd\n"],"metadata":{"id":"MU-X3Mh5JyY4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the California Housing Dataset\n","housing = fetch_california_housing()\n","\n","# Convert the dataset to a DataFrame for easier exploration\n","data = pd.DataFrame(housing.data, columns=housing.feature_names)\n","data['Target'] = housing.target\n","\n","# Display the first few rows\n","print(data.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7M_TdXLHHL1B","executionInfo":{"status":"ok","timestamp":1734414486350,"user_tz":-330,"elapsed":2053,"user":{"displayName":"Sujoy Sarkar Sujoy Sarkar","userId":"17509397499268237284"}},"outputId":"0c37f643-0d88-4ebd-fce1-a2eb0a6d2b1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n","0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n","1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n","2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n","3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n","4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n","\n","   Longitude  Target  \n","0    -122.23   4.526  \n","1    -122.22   3.585  \n","2    -122.24   3.521  \n","3    -122.25   3.413  \n","4    -122.25   3.422  \n"]}]}]}