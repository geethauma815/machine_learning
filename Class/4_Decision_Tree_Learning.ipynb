{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **<font color='darkorange'>Decision Tree Learning Tutorial</font>**\n","\n","---\n","\n","## **<font color='darkorange'>1. Introduction to Decision Tree Algorithm</font>**\n","\n","A **Decision Tree** is one of the most popular machine learning algorithms. It uses a **tree-like structure** to represent decisions and their possible outcomes.  \n","\n","### **Key Features**:\n","- **Supervised Learning**: It can be used for **Classification** and **Regression** tasks.\n","- **Tree Representation**: A decision tree consists of:\n","  - **Root Node**: The starting point.\n","  - **Internal Nodes**: Represent tests on attributes.\n","  - **Branches**: Denote outcomes of the tests.\n","  - **Leaf Nodes**: Represent class labels or regression outputs.\n","\n","<img  src=\"https://drive.google.com/uc?id=1WOpBBe9pX9uAPZKzepKfP2iseTkhMyhs\"/>\n","\n","### **Assumptions**:\n","1. The whole training set is considered as the **root** at the start.\n","2. Feature values need to be **categorical**. If values are continuous, they are **discretized**.\n","3. Records are split **recursively** based on attribute values.\n","4. The order of placing attributes is determined using **statistical measures** like **Gini Index** or **Entropy**.\n","\n","---\n","\n","## **<font color='darkorange'>Classification and Regression Trees (CART)</font>**\n","\n","The modern name for decision trees is **CART**, which stands for **Classification and Regression Trees**.\n","\n","### **What is CART?**\n","- **Classification**: Predicts a **categorical outcome**.  \n","- **Regression**: Predicts a **continuous outcome**.  \n","\n","### **History**:\n","The term **CART** was introduced by **Leo Breiman**, and it forms the basis for advanced ensemble algorithms like:\n","- **Bagged Trees**\n","- **Random Forests**\n","- **Boosted Trees**\n","\n","---\n","## **<font color='darkorange'>Classification Tree Example</font>**\n","\n","<img src=\"https://drive.google.com/uc?id=18fn15kdDpYto-fbnUeiP_REDMwiLrdYC\" width=700/>\n","\n","---\n","## **<font color='darkorange'>Regression Tree Example</font>**\n","\n","<img src=\"https://drive.google.com/uc?id=1lLgkWbcUX4ggRRKtMYVPuJW7tnQFG1nM\" width=700/>\n","---\n"],"metadata":{"id":"TF_ScNhFXEnJ"}},{"cell_type":"markdown","source":["# **<font color='darkorange'>Decision Tree Learning Tutorial</font>**\n","\n","---\n","\n","## **<font color='darkorange'>1. Steps to Create a Decision Tree</font>**\n","\n","To create a **Decision Tree**, follow these steps:\n","\n","### **Step 1: Start with the Entire Dataset**\n","- Treat the entire dataset as the **root node**.\n","\n","### **Step 2: Choose the Best Attribute for Splitting**\n","- Use a **splitting criterion** to determine the best attribute to split the data.  \n","- Examples of criteria: **Gini Index**, **Entropy**, or **Mean Squared Error (MSE)**.\n","\n","### **Step 3: Split the Dataset**\n","- Split the dataset into child nodes based on the selected attribute's values.\n","\n","### **Step 4: Recursively Repeat the Process**\n","- Treat each child node as a new dataset and repeat the process:\n","  - Choose the best attribute.  \n","  - Split further until a **stopping condition** is met.\n","\n","### **Step 5: Assign Class Labels or Values to Leaf Nodes**\n","- For **classification**: Assign the most common class in the node.  \n","- For **regression**: Assign the mean of the target values in the node.\n","\n","---\n","\n","## **<font color='darkorange'>2. Splitting Criteria in Decision Trees</font>**\n","\n","The **splitting criterion** determines how to split the dataset at each node. Different criteria are used for **classification** and **regression** trees.\n","\n","---\n","\n","### **<font color='blue'>2.1 Gini Index (for Classification)</font>**\n","\n","The **Gini Index** measures the **impurity** of a node. A **lower Gini Index** means the node is purer.\n","\n","#### **Formula**:\n","$$\n","Gini = 1 - \\sum_{i=1}^c p_i^2\n","$$\n","Where:\n","- $ p_i $ is the probability of class $ i $ in the node.  \n","- $ c $ is the number of classes.\n","\n","#### **Example**:\n","If a node contains 4 samples: 3 in Class A and 1 in Class B:  \n","$$\n","Gini = 1 - \\left( \\frac{3}{4} \\right)^2 - \\left( \\frac{1}{4} \\right)^2 = 0.375\n","$$\n","\n","---\n","\n","### **<font color='blue'>2.2 Entropy and Information Gain (for Classification)</font>**\n","\n","**Entropy** measures the level of **disorder** in the data. The goal is to minimize entropy using **Information Gain**.\n","\n","#### **Formula for Entropy**:\n","$$\n","Entropy = -\\sum_{i=1}^c p_i \\log_2(p_i)\n","$$\n","Where:\n","- $ p_i $ is the probability of class $ i $ in the node.\n","\n","#### **Formula for Information Gain**:\n","$$\n","Information\\ Gain = Entropy_{parent} - \\sum_{j} \\left( \\frac{n_j}{N} \\times Entropy_{child\\ j} \\right)\n","$$\n","Where:\n","- $ n_j $ is the number of samples in child node $ j $.  \n","- $ N $ is the total number of samples in the parent node.\n","\n","**Goal**: Choose the attribute that **maximizes** the Information Gain.\n","\n","---\n","\n","### **<font color='blue'>2.3 Mean Squared Error (MSE) for Regression</font>**\n","\n","For regression trees, **Mean Squared Error (MSE)** measures the quality of a split. The goal is to **minimize** MSE.\n","\n","#### **Formula**:\n","$$\n","MSE = \\frac{\\sum_{i=1}^n (y_i - \\hat{y})^2}{n}\n","$$\n","Where:\n","- $ y_i $ is the actual value.  \n","- $ \\hat{y} $ is the predicted value (mean of the target values in the node).  \n","- $ n $ is the number of samples in the node.\n","\n","---\n","\n","## **<font color='darkorange'>3. Stopping Conditions for Building Decision Trees</font>**\n","\n","The splitting process stops when one of the following conditions is met:\n","\n","1. **Pure Node**: All samples in a node belong to the same class.\n","2. **Maximum Depth**: The tree has reached a pre-defined depth.\n","3. **Minimum Samples**: A node contains fewer samples than a threshold.\n","4. **No Improvement**: Splitting the node does not improve the model's performance.\n","\n","---\n","\n","## **<font color='darkorange'>4. Practical Example: Steps to Create a Decision Tree</font>**\n","\n","### **Example Dataset: Classification**\n","| **Age (Years)** | **Income (USD)** | **Student** | **Buys Product (Yes/No)** |\n","|-----------------|------------------|-------------|---------------------------|\n","| 22             | 30,000           | Yes         | Yes                       |\n","| 25             | 40,000           | No          | No                        |\n","| 47             | 50,000           | No          | Yes                       |\n","| 35             | 60,000           | Yes         | Yes                       |\n","| 52             | 70,000           | No          | No                        |\n","\n","---\n","\n","### **Steps**:\n","1. Start with the **root node**.\n","2. Calculate **Gini Index** or **Entropy** for all features:  \n","   - Age, Income, and Student status.\n","3. Split the data on the feature with the **lowest Gini Index** or **highest Information Gain**.\n","4. Continue recursively until stopping conditions are met.\n","5. Assign **class labels** at the leaf nodes.\n","\n","---\n","\n","### **Code for Classification Tree**:\n","\n","```python\n","# Import required libraries\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Step 1: Create the dataset\n","data = {\n","    'Age': [22, 25, 47, 35, 52],\n","    'Income': [30000, 40000, 50000, 60000, 70000],\n","    'Student': [1, 0, 0, 1, 0],  # Yes = 1, No = 0\n","    'Buys': ['Yes', 'No', 'Yes', 'Yes', 'No']\n","}\n","df = pd.DataFrame(data)\n","\n","# Step 2: Define features (X) and target (y)\n","X = df[['Age', 'Income', 'Student']]\n","y = df['Buys']\n","\n","# Step 3: Train the Decision Tree Classifier\n","classifier = DecisionTreeClassifier(criterion='gini', max_depth=3)\n","classifier.fit(X, y)\n","\n","# Step 4: Visualize the Decision Tree\n","plt.figure(figsize=(12, 8))\n","plot_tree(classifier, feature_names=X.columns, class_names=['No', 'Yes'], filled=True, rounded=True)\n","plt.title(\"Decision Tree for Classification Example\")\n","plt.show()\n"],"metadata":{"id":"esku9mVzelSo"}},{"cell_type":"code","source":[],"metadata":{"id":"BvxJQS-xbR1h"},"execution_count":null,"outputs":[]}]}