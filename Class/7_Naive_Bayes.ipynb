{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üéØ <span style=\"color:#3498db; font-weight:bold;\">Understanding Probability Concepts in Bayesian Classification</span>\n","\n","Before learning **Bayesian Classification**, we need to understand the fundamental probability concepts:  \n","- **Joint Probability**\n","- **Conditional Probability**\n","- **Marginal Probability**  \n","These concepts help in constructing **Bayes‚Äô Theorem**, which is the foundation of **Na√Øve Bayes Classifier**.\n","\n","---\n","\n","## üìä <span style=\"color:#e74c3c;\">Example Dataset: Employees' Gender and Job Class</span>\n","We have collected data on **employees** based on **two variables**:  \n","- **Gender (M, F)**\n","- **Class (C1, C2, C3)**\n","\n","| Gender | C1  | C2  | C3  | Total |\n","|--------|----|----|----|------|\n","| **M**  | 30 | 15 | 55 | 100  |\n","| **F**  | 10 | 15 | 25 | 50   |\n","| **Total** | 40 | 30 | 80 | 150  |\n","\n","From this table:\n","- There are **150 employees in total**.\n","- **100 males** and **50 females**.\n","- Employees are categorized into **three job classes** (**C1, C2, C3**).\n","\n","---\n","\n","## üîπ <span style=\"color:#16a085;\">1. Joint Probability</span>\n","**Definition:**  \n","**Joint probability** measures the likelihood of **two events happening together**.\n","\n","$$\n","P(A \\cap B) = \\frac{\\text{Number of occurrences of (A, B)}}{\\text{Total observations}}\n","$$\n","\n","### üí° <span style=\"color:#f39c12;\">Example from Dataset</span>\n","- Probability that a randomly chosen employee is **Male and belongs to Class C1**:\n","\n","$$\n","P(M \\cap C1) = \\frac{30}{150} = 0.20\n","$$\n","\n","- Probability that an employee is **Female and in Class C3**:\n","\n","$$\n","P(F \\cap C3) = \\frac{25}{150} = 0.1667\n","$$\n","\n","---\n","\n","## üîπ <span style=\"color:#9b59b6;\">2. Marginal Probability</span>\n","**Definition:**  \n","**Marginal probability** is the probability of **a single event occurring**, regardless of any other event.\n","\n","$$\n","P(A) = \\frac{\\text{Total occurrences of A}}{\\text{Total observations}}\n","$$\n","\n","### üí° <span style=\"color:#f39c12;\">Example from Dataset</span>\n","- Probability that an employee is **Male**:\n","\n","$$\n","P(M) = \\frac{100}{150} = 0.6667\n","$$\n","\n","- Probability that an employee belongs to **Class C2**:\n","\n","$$\n","P(C2) = \\frac{30}{150} = 0.20\n","$$\n","\n","---\n","\n","## üîπ <span style=\"color:#c0392b;\">3. Conditional Probability</span>\n","**Definition:**  \n","Conditional probability is the probability of **event A occurring, given that event B has already occurred**.\n","\n","$$\n","P(A | B) = \\frac{P(A \\cap B)}{P(B)}\n","$$\n","\n","### üí° <span style=\"color:#f39c12;\">Example from Dataset</span>\n","- Probability that an employee is **Male, given they belong to Class C1**:\n","\n","$$\n","P(M | C1) = \\frac{P(M \\cap C1)}{P(C1)}\n","$$\n","\n","$$\n","P(M | C1) = \\frac{30}{40} = 0.75\n","$$\n","\n","- Probability that an employee is **Female, given they belong to Class C3**:\n","\n","$$\n","P(F | C3) = \\frac{P(F \\cap C3)}{P(C3)}\n","$$\n","\n","$$\n","P(F | C3) = \\frac{25}{80} = 0.3125\n","$$\n","\n","---\n","\n","## üîÅ <span style=\"color:#8e44ad;\">4. Expressing One Probability in Terms of Another</span>\n","Each type of probability can be rewritten in terms of the others.\n","\n","### üü¢ **Marginal Probability Using Joint Probability**\n","Marginal probability can be found by **summing** joint probabilities.\n","\n","$$\n","P(A) = \\sum_B P(A \\cap B)\n","$$\n","\n","**Example:**  \n","To get the probability of **C1**:\n","\n","$$\n","P(C1) = P(M \\cap C1) + P(F \\cap C1)\n","$$\n","\n","$$\n","P(C1) = \\frac{30}{150} + \\frac{10}{150} = \\frac{40}{150} = 0.2667\n","$$\n","\n","---\n","\n","### üîµ **Joint Probability Using Conditional and Marginal Probability**\n","Using the **definition of conditional probability**:\n","\n","$$\n","P(A \\cap B) = P(A | B) \\cdot P(B)\n","$$\n","\n","**Example:**  \n","To find $ P(M \\cap C1) $:\n","\n","$$\n","P(M \\cap C1) = P(M | C1) \\cdot P(C1)\n","$$\n","\n","$$\n","P(M \\cap C1) = 0.75 \\times 0.2667 = 0.20\n","$$\n","\n","---\n","\n","### üü† **Conditional Probability Using Joint and Marginal Probability**\n","Rearranging the joint probability equation:\n","\n","$$\n","P(A | B) = \\frac{P(A \\cap B)}{P(B)}\n","$$\n","\n","**Example:**  \n","To find $ P(F | C3) $:\n","\n","$$\n","P(F | C3) = \\frac{P(F \\cap C3)}{P(C3)}\n","$$\n","\n","$$\n","P(F | C3) = \\frac{0.1667}{0.5333} = 0.3125\n","$$\n","\n","---\n","\n","# üîÆ <span style=\"color:#2c3e50;\">5. Bayes' Theorem</span>\n","Now, using the relationships we established, we derive **Bayes' Rule**, which is the basis for Bayesian Classification.\n","\n","$$\n","P(A | B) = \\frac{P(B | A) P(A)}{P(B)}\n","$$\n","\n","This equation allows us to **reverse conditional probabilities**, meaning we can compute **P(A | B)** from **P(B | A)**.\n","\n","## üîÆ <span style=\"color:#2c3e50;\">Derivation of Bayes' Theorem</span>\n","\n","### üîπ **Starting from the Definition of Conditional Probability**\n","By definition, the conditional probability of **A given B** is:\n","\n","$$\n","P(A | B) = \\frac{P(A \\cap B)}{P(B)}\n","$$\n","\n","Similarly, the conditional probability of **B given A** is:\n","\n","$$\n","P(B | A) = \\frac{P(A \\cap B)}{P(A)}\n","$$\n","\n","Since **P(A ‚à© B)** (the joint probability of A and B) is the same in both equations, we equate them:\n","\n","$$\n","P(A | B) \\cdot P(B) = P(B | A) \\cdot P(A)\n","$$\n","\n","Rearranging for **P(A | B):**\n","\n","$$\n","P(A | B) = \\frac{P(B | A) P(A)}{P(B)}\n","$$\n","\n","This is the **Bayes' Theorem**, which allows us to reverse conditional probabilities.\n","\n","---\n","\n","# üìå <span style=\"color:#16a085;\">Understanding the Terms in Bayes' Theorem</span>\n","\n","<img src=\"https://drive.google.com/uc?id=1KBETj61Usb_ra3QCvjM5KyNG6_uPqrVa\"/>\n","\n","- **$P(A)$ - Prior Probability**  \n","  - This is the initial probability of **A** before considering evidence **B**.  \n","  - It represents what we already know about **A** independently.  \n","\n","- **$P(A | B)$ - Posterior Probability**  \n","  - This is the probability of **A given B**.  \n","  - It is called the **posterior probability** because it is calculated after incorporating the new evidence **B**.  \n","\n","- **$P(B | A)$ - Likelihood**  \n","  - This is the probability of **B given A**.  \n","  - It represents how likely we are to observe **B**, assuming **A** is true.  \n","\n","- **$P(B)$ - Evidence - Normalizing Constant**  \n","  - This is the **prior probability of B**, ensuring that probabilities sum to 1.  \n","  - It acts as a **scaling factor** to adjust for all possible cases.  \n","\n","---\n","\n","# üí° <span style=\"color:#f39c12;\">Example: Predicting Gender Given Job Class</span>\n","Using our employee dataset, let‚Äôs compute:\n","\n","**What is the probability that an employee is Male given they belong to Class C1?**  \n","That is, we want to find **P(M | C1)**.\n","\n","From our dataset:\n","- **Prior Probability** of Male:  \n","  $$ P(M) = \\frac{100}{150} = 0.6667 $$\n","\n","- **Likelihood** of Class C1 given Male:  \n","  $$ P(C1 | M) = \\frac{30}{100} = 0.30 $$\n","\n","- **Marginal Probability** of Class C1:  \n","  $$ P(C1) = \\frac{40}{150} = 0.2667 $$\n","\n","Applying **Bayes' Theorem**:\n","\n","$$\n","P(M | C1) = \\frac{P(C1 | M) P(M)}{P(C1)}\n","$$\n","\n","$$\n","P(M | C1) = \\frac{0.30 \\times 0.6667}{0.2667}\n","$$\n","\n","$$\n","P(M | C1) = \\frac{0.20}{0.2667} = 0.75\n","$$\n","\n","- üîπ **Conclusion:** Given that an employee belongs to **Class C1**, there is a **75% probability** that they are **Male**.\n","---\n","\n","<img src=\"https://drive.google.com/uc?id=13knr-2KjZ944PNEGdWkMBgx-8dlJEw3L\"/>\n","\n","---\n"],"metadata":{"id":"8rf0m0C1L2Jd"}},{"cell_type":"markdown","source":["# üìå **Example 1:**\n","### **Problem Statement**\n","We have two bags:\n","\n","- **Bag I** contains **4 white** and **6 black** balls.\n","- **Bag II** contains **4 white** and **3 black** balls.\n","- A bag is chosen **at random**, and **one ball is drawn**.\n","- The ball drawn is **black**.\n","- **What is the probability that it came from Bag I?**\n","\n","---\n","\n","### **Step 1: Define Events**\n","- **Let \\( A_1 \\) be the event of choosing Bag I**.\n","- **Let \\( A_2 \\) be the event of choosing Bag II**.\n","- **Let \\( B \\) be the event of drawing a black ball**.\n","\n","We are given:\n","- **Since any bag is equally likely to be chosen**:  \n","  $$\n","  P(A_1) = P(A_2) = 0.5\n","  $$\n","\n","- **Probability of drawing a black ball from Bag I**:  \n","  $$\n","  P(B | A_1) = \\frac{6}{10} = 0.6\n","  $$\n","\n","- **Probability of drawing a black ball from Bag II**:  \n","  $$\n","  P(B | A_2) = \\frac{3}{7} \\approx 0.4286\n","  $$\n","\n","---\n","\n","### **Step 2: Compute Total Probability of Drawing a Black Ball (\\( P(B) \\))**\n","Using the **law of total probability**:\n","\n","$$\n","P(B) = P(B | A_1) P(A_1) + P(B | A_2) P(A_2)\n","$$\n","\n","Substituting values:\n","\n","$$\n","P(B) = (0.6 \\times 0.5) + (0.4286 \\times 0.5)\n","$$\n","\n","$$\n","P(B) = 0.3 + 0.2143 = 0.5143\n","$$\n","\n","---\n","\n","### **Step 3: Compute \\( P(A_1 | B) \\) Using Bayes‚Äô Theorem**\n","Applying Bayes‚Äô Rule:\n","\n","$$\n","P(A_1 | B) = \\frac{P(B | A_1) P(A_1)}{P(B)}\n","$$\n","\n","Substituting values:\n","\n","$$\n","P(A_1 | B) = \\frac{(0.6 \\times 0.5)}{0.5143}\n","$$\n","\n","$$\n","P(A_1 | B) = \\frac{0.3}{0.5143} \\approx 0.5837\n","$$\n","\n","üîπ **Conclusion:**  \n","If a **black ball** is drawn, the probability that it came from **Bag I** is **58.37%**.\n","\n","---\n","\n","# üìå **Example 2:**\n","We want to find the probability that **it rained on Sunday, given that it rained on Monday**.\n","\n","---\n","\n","### **Step 1: Define Events**\n","- **Let \\( A \\) be the event that it rains on Sunday.**\n","- **Let \\( B \\) be the event that it rains on Monday.**\n","\n","We are given:\n","- **Prior probability of rain on Sunday**:\n","  $$\n","  P(A) = 0.40\n","  $$\n","- **Probability of rain on Monday given it rained on Sunday**:\n","  $$\n","  P(B | A) = 0.10\n","  $$\n","- **Probability of rain on Monday given it did NOT rain on Sunday**:\n","  $$\n","  P(B | A') = 0.80\n","  $$\n","\n","---\n","\n","### **Step 2: Compute Total Probability of Rain on Monday (\\( P(B) \\))**\n","Using the **law of total probability**:\n","\n","$$\n","P(B) = P(B | A) P(A) + P(B | A') P(A')\n","$$\n","\n","Since **\\( P(A') = 1 - P(A) = 0.60 \\)**, we get:\n","\n","$$\n","P(B) = (0.10 \\times 0.40) + (0.80 \\times 0.60)\n","$$\n","\n","$$\n","P(B) = 0.04 + 0.48 = 0.52\n","$$\n","\n","---\n","\n","### **Step 3: Compute \\( P(A | B) \\) Using Bayes‚Äô Theorem**\n","Applying Bayes‚Äô Rule:\n","\n","$$\n","P(A | B) = \\frac{P(B | A) P(A)}{P(B)}\n","$$\n","\n","Substituting values:\n","\n","$$\n","P(A | B) = \\frac{(0.10 \\times 0.40)}{0.52}\n","$$\n","\n","$$\n","P(A | B) = \\frac{0.04}{0.52} = 0.0769\n","$$\n","\n","üîπ **Conclusion:**  \n","If it **rained on Monday**, the probability that it **also rained on Sunday** is **7.69%**.\n","\n","---"],"metadata":{"id":"pWwhQ9ERU06z"}},{"cell_type":"markdown","source":["# üéØ **Bayes‚Äô Theorem in Machine Learning**\n","\n","Bayes' Theorem is widely used in **Machine Learning**, particularly in **classification problems**, to **update probabilities** based on new data. One of the most common applications of Bayes' Theorem in ML is the **Na√Øve Bayes Classifier**.\n","\n","---\n","\n","## üîπ **How is Bayes‚Äô Theorem Used in Machine Learning?**\n","In **classification problems**, we aim to assign an instance **$ \\mathbf{x} $** to a particular class **$ C_k $**, based on given features.\n","\n","Using **Bayes' Theorem**, we can compute the probability that a given instance **belongs to class $ C_k $**, given its feature values:\n","\n","$$\n","P(C_k | \\mathbf{x}) = \\frac{P(C_k) P(\\mathbf{x} | C_k)}{P(\\mathbf{x})}\n","$$\n","\n","where:\n","- **$ P(C_k | \\mathbf{x}) $** = **Posterior Probability** (Probability of class $ C_k $ given the features $ \\mathbf{x} $)\n","- **$ P(C_k) $** = **Prior Probability** (Initial belief about class $ C_k $)\n","- **$ P(\\mathbf{x} | C_k) $** = **Likelihood** (Probability of observing the feature set $ \\mathbf{x} $ if the instance belongs to $ C_k $)\n","- **$ P(\\mathbf{x}) $** = **Evidence** (Total probability of $ \\mathbf{x} $ occurring across all classes)\n","\n","Since **$ P(\\mathbf{x}) $** is a normalizing constant and does not depend on $ C_k $, we focus on:\n","\n","$$\n","P(C_k | \\mathbf{x}) \\propto P(C_k) P(\\mathbf{x} | C_k)\n","$$\n","\n","---\n","\n","## üîπ **Example: Playing Golf Decision Based on Weather Conditions**\n","Imagine we want to predict whether a person will **play golf ($Y$) or not ($N$)** based on the weather conditions.\n","\n","### **Features Affecting the Decision:**\n","1. **Outlook** (Sunny, Overcast, Rainy)\n","2. **Temperature** (Hot, Mild, Cool)\n","3. **Humidity** (High, Normal)\n","4. **Wind** (Weak, Strong)\n","\n","### **How Each Term Maps to the Problem**\n","- **$ P(Y) $**: The probability of playing golf before considering the weather conditions (**Prior Probability**).  \n","- **$ P(\\text{Outlook = Sunny} | Y) $**: The probability of a **Sunny day** given that the person plays golf (**Likelihood**).  \n","- **$ P(\\text{Outlook = Sunny}) $**: The overall probability of a **Sunny day** occurring (**Evidence**).  \n","- **$ P(Y | \\text{Outlook = Sunny}) $**: The updated probability of **playing golf** given that it is **Sunny** (**Posterior Probability**).  \n","\n","Using Bayes' Theorem, we update our belief about whether the person will play golf based on the weather.\n","\n","---\n","\n","# üîπ **Concept of Independence & Na√Øve Assumption**\n","The key assumption in **Na√Øve Bayes** is that **all features $ x_i $ are conditionally independent given the class $ C_k $**.\n","\n","### **Example: Na√Øve Assumption in the Golf Problem**\n","Suppose we want to compute:\n","\n","$$\n","P(Y | \\text{Outlook=Sunny, Temperature=Hot, Humidity=High, Wind=Weak})\n","$$\n","\n","A **normal classifier** would consider how these weather conditions interact with each other. But **Na√Øve Bayes assumes that each feature contributes independently** to the probability:\n","\n","$$\n","P(\\text{Outlook=Sunny, Temperature=Hot, Humidity=High, Wind=Weak} | Y)\n","$$\n","\n","Applying the **Na√Øve Assumption (Conditional Independence)**:\n","\n","$$\n","P(\\text{Outlook=Sunny} | Y) P(\\text{Temperature=Hot} | Y) P(\\text{Humidity=High} | Y) P(\\text{Wind=Weak} | Y)\n","$$\n","\n","This **simplifies** calculations but may not always be **100% realistic** because weather conditions often **depend on each other**. However, **Na√Øve Bayes still performs well in practice**, especially for text classification.\n","\n","---\n","\n","# üîπ **Final Na√Øve Bayes Formula**\n","Using the **Na√Øve Assumption**, the final probability of a class $ C_k $ given feature set $ \\mathbf{x} $ is:\n","\n","$$\n","P(C_k | \\mathbf{x}) \\propto P(C_k) \\prod_{i=1}^{n} P(x_i | C_k)\n","$$\n","\n","where:\n","- **$ P(C_k) $** = Prior probability of class $ C_k $\n","- **$ P(x_i | C_k) $** = Probability of feature $ x_i $ given class $ C_k $\n","\n","---\n","\n","# üîπ **Final Prediction Rule**\n","To classify a new instance $ \\mathbf{x} $, we compute:\n","\n","$$\n","y_{\\text{pred}} = \\arg\\max_{C_k} P(C_k) \\prod_{i=1}^{n} P(x_i | C_k)\n","$$\n","\n","üîπ **Conclusion:**  \n","- **Na√Øve Bayes is simple, fast, and effective**, especially for **text classification** (spam detection, sentiment analysis, etc.).  \n","- The **Na√Øve assumption** of **feature independence** may not always hold, but Na√Øve Bayes still performs well in practice.\n"],"metadata":{"id":"MbUpLAxLYKtD"}},{"cell_type":"markdown","source":["# üéØ **Na√Øve Bayes Classification: Numerical Example**\n","\n","We will apply **Na√Øve Bayes Classification** to predict whether a person will **play golf ($ Y $) or not ($ N $)** given the **weather conditions**.\n","\n","<img src=\"https://drive.google.com/uc?id=1lf6VOdjQDbwu4l8z_mWUEb_xYaZw8MQb\"/>\n","---\n","\n","## üîπ **Step 1: Understanding the Formula**\n","Using **Bayes' Theorem**, the probability of class **$ y $** given features **$ \\mathbf{X} = (x_1, x_2, ..., x_n) $** is:\n","\n","$$\n","P(y | \\mathbf{X}) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i | y)}{P(\\mathbf{X})}\n","$$\n","\n","Since **$ P(\\mathbf{X}) $** is the same for all classes, we can ignore it:\n","\n","$$\n","P(y | \\mathbf{X}) \\propto P(y) \\prod_{i=1}^{n} P(x_i | y)\n","$$\n","\n","We compute **$ P(Y | \\mathbf{X}) $** and **$ P(N | \\mathbf{X}) $**, and classify based on the higher probability.\n","\n","---\n","\n","## üîπ **Step 2: Given Data from the Dataset**\n","We have computed probabilities from the dataset:\n","\n","<img src=\"https://drive.google.com/uc?id=1XljrXkHEXeI1cDEneFw1mfUi_R9cOUTY\"/>\n","\n","- **Class Probabilities:**\n","  - $ P(Y) = \\frac{9}{14} $\n","  - $ P(N) = \\frac{5}{14} $\n","\n","- **Conditional Probabilities from the Dataset:**\n","  - **For $ Y $ (Play Golf = Yes):**\n","    - $ P(\\text{Sunny} | Y) = \\frac{2}{9} $\n","    - $ P(\\text{Hot} | Y) = \\frac{2}{9} $\n","    - $ P(\\text{Normal} | Y) = \\frac{6}{9} $\n","    - $ P(\\text{Weak} | Y) = \\frac{6}{9} $\n","  \n","  - **For $ N $ (Play Golf = No):**\n","    - $ P(\\text{Sunny} | N) = \\frac{3}{5} $\n","    - $ P(\\text{Hot} | N) = \\frac{2}{5} $\n","    - $ P(\\text{Normal} | N) = \\frac{1}{5} $\n","    - $ P(\\text{Weak} | N) = \\frac{2}{5} $\n","\n","---\n","\n","## üîπ **Step 3: Compute Posterior Probabilities**\n","We now classify **today‚Äôs weather: (Sunny, Hot, Normal, Weak)**.\n","\n","### **Calculate $ P(Y | \\text{today}) $**\n","Using:\n","\n","$$\n","P(Y | \\text{today}) \\propto P(Y) P(\\text{Sunny} | Y) P(\\text{Hot} | Y) P(\\text{Normal} | Y) P(\\text{Weak} | Y)\n","$$\n","\n","Substituting values:\n","\n","$$\n","P(Y | \\text{today}) \\propto \\left(\\frac{9}{14}\\right) \\times \\left(\\frac{2}{9}\\right) \\times \\left(\\frac{2}{9}\\right) \\times \\left(\\frac{6}{9}\\right) \\times \\left(\\frac{6}{9}\\right)\n","$$\n","\n","$$\n","P(Y | \\text{today}) \\propto \\frac{9 \\times 2 \\times 2 \\times 6 \\times 6}{14 \\times 9 \\times 9 \\times 9 \\times 9}\n","$$\n","\n","$$\n","P(Y | \\text{today}) \\propto 0.02116\n","$$\n","\n","---\n","\n","### **Calculate $ P(N | \\text{today}) $**\n","Using:\n","\n","$$\n","P(N | \\text{today}) \\propto P(N) P(\\text{Sunny} | N) P(\\text{Hot} | N) P(\\text{Normal} | N) P(\\text{Weak} | N)\n","$$\n","\n","Substituting values:\n","\n","$$\n","P(N | \\text{today}) \\propto \\left(\\frac{5}{14}\\right) \\times \\left(\\frac{3}{5}\\right) \\times \\left(\\frac{2}{5}\\right) \\times \\left(\\frac{1}{5}\\right) \\times \\left(\\frac{2}{5}\\right)\n","$$\n","\n","$$\n","P(N | \\text{today}) \\propto \\frac{5 \\times 3 \\times 2 \\times 1 \\times 2}{14 \\times 5 \\times 5 \\times 5 \\times 5}\n","$$\n","\n","$$\n","P(N | \\text{today}) \\propto 0.0068\n","$$\n","\n","---\n","\n","## üîπ **Step 4: Normalize and Compare Probabilities**\n","Since:\n","\n","$$\n","P(Y | \\text{today}) > P(N | \\text{today})\n","$$\n","\n","We classify **today‚Äôs weather** as:\n","\n","$$\n","\\textbf{Play Golf (Yes)}\n","$$\n","\n","---\n","\n","# üèÜ **Conclusion**\n","Using **Na√Øve Bayes Classification**, we predicted that the person will **play golf today** based on the given weather conditions.\n","\n","üöÄ **Next Step:** Implement this in Python using **scikit-learn**!\n"],"metadata":{"id":"cGGNAiGRbh0m"}},{"cell_type":"markdown","source":["## üîπ **Normalize Probability**\n","$ P(\\text{today}) $\n","The total probability of today's weather conditions occurring is:\n","\n","$$\n","P(\\text{today}) = P(Y) P(\\text{Sunny} | Y) P(\\text{Hot} | Y) P(\\text{Normal} | Y) P(\\text{Weak} | Y) + P(N) P(\\text{Sunny} | N) P(\\text{Hot} | N) P(\\text{Normal} | N) P(\\text{Weak} | N)\n","$$\n","\n","Substituting the values:\n","\n","$$\n","P(\\text{today}) = (0.02116) + (0.0068)\n","$$\n","\n","$$\n","P(\\text{today}) = 0.02796\n","$$\n","\n","Now, we compute the **actual probabilities** by normalizing:\n","\n","$$\n","P(Y | \\text{today}) = \\frac{P(Y) P(\\text{Sunny} | Y) P(\\text{Hot} | Y) P(\\text{Normal} | Y) P(\\text{Weak} | Y)}{P(\\text{today})}\n","$$\n","\n","$$\n","P(Y | \\text{today}) = \\frac{0.02116}{0.02796} \\approx 0.7565\n","$$\n","\n","$$\n","P(N | \\text{today}) = \\frac{P(N) P(\\text{Sunny} | N) P(\\text{Hot} | N) P(\\text{Normal} | N) P(\\text{Weak} | N)}{P(\\text{today})}\n","$$\n","\n","$$\n","P(N | \\text{today}) = \\frac{0.0068}{0.02796} \\approx 0.2435\n","$$\n","\n","---\n","\n","## üîπ **Final Decision**\n","Since:\n","\n","$$\n","P(Y | \\text{today}) > P(N | \\text{today})\n","$$\n","\n","We classify **today‚Äôs weather** as:\n","\n","$$\n","\\textbf{Play Golf (Yes)}\n","$$\n","\n","Now, we have fully normalized probabilities that sum to 1:\n","\n","$$\n","P(Y | \\text{today}) + P(N | \\text{today}) = 1\n","$$\n","\n","---\n"],"metadata":{"id":"bz4WbGFEexvl"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import classification_report\n","\n","# Creating the dataset\n","data = {\n","    'Outlook': ['Rainy', 'Rainy', 'Overcast', 'Sunny', 'Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Sunny',\n","                'Rainy', 'Overcast', 'Overcast', 'Sunny'],\n","    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild',\n","                    'Mild', 'Mild', 'Hot', 'Mild'],\n","    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal',\n","                 'Normal', 'High', 'Normal', 'High'],\n","    'Windy': ['False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False',\n","              'True', 'True', 'False', 'True'],\n","    'Play Golf': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes',\n","                  'Yes', 'Yes', 'Yes', 'No']\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Encoding categorical variables\n","encoder = LabelEncoder()\n","for col in df.columns:\n","    df[col] = encoder.fit_transform(df[col])\n","\n","# Splitting features and target variable\n","X = df.drop(columns=['Play Golf'])\n","y = df['Play Golf']\n","\n","# Training the Multinomial Na√Øve Bayes model\n","model = MultinomialNB()\n","model.fit(X, y)\n","\n","# Predicting on training data\n","y_pred = model.predict(X)\n","\n","# Generating classification report\n","print(\"Classification Report:\\n\", classification_report(y, y_pred, target_names=['No', 'Yes']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wfMpGhW9L3pe","executionInfo":{"status":"ok","timestamp":1742137098140,"user_tz":-330,"elapsed":378,"user":{"displayName":"Sujoy Sarkar Sujoy Sarkar","userId":"17509397499268237284"}},"outputId":"5b98c2c9-47ee-4b7c-def7-5ad9476cbecf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification Report:\n","               precision    recall  f1-score   support\n","\n","          No       1.00      0.40      0.57         5\n","         Yes       0.75      1.00      0.86         9\n","\n","    accuracy                           0.79        14\n","   macro avg       0.88      0.70      0.71        14\n","weighted avg       0.84      0.79      0.76        14\n","\n"]}]},{"cell_type":"markdown","source":["# üéØ **Scikit-learn Implementations of Na√Øve Bayes**\n","Scikit-learn provides four implementations of Na√Øve Bayes, each differing in **assumptions about the feature distribution**:\n","\n","1. **Gaussian Na√Øve Bayes (`GaussianNB`)** ‚Äì Assumes features follow a **normal (Gaussian) distribution**.\n","2. **Multinomial Na√Øve Bayes (`MultinomialNB`)** ‚Äì Used for **count-based** data (e.g., text classification).\n","3. **Bernoulli Na√Øve Bayes (`BernoulliNB`)** ‚Äì Used for **binary features** (e.g., presence/absence of words).\n","4. **Complement Na√Øve Bayes (`ComplementNB`)** ‚Äì A variant of `MultinomialNB`, designed for **imbalanced datasets**.\n","\n","---\n","\n","## üîπ **Gaussian Na√Øve Bayes**\n","Gaussian Na√Øve Bayes assumes that **features are continuous** and follow a **normal distribution**:\n","\n","<img src=\"https://drive.google.com/uc?id=1qBMsnlUtmGywIJhHYf6jEnxEZQTqhamB\" width=600>\n","\n","\n","$$\n","P(x = v | C_k) = \\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}} \\exp \\left( -\\frac{(v - \\mu_k)^2}{2 \\sigma_k^2} \\right)\n","$$\n","\n","where:\n","- $ \\mu_k $ is the **mean** of the feature for class $ C_k $.\n","- $ \\sigma_k^2 $ is the **variance** of the feature for class $ C_k $.\n","- $ v $ is the observed value of the feature.\n","\n","The parameters $ \\mu_k $ and $ \\sigma_k^2 $ are estimated using **maximum likelihood estimation**.\n","\n","---\n","\n","## üîπ **Gender Classification Example**\n","### **Problem Statement**\n","We want to classify whether a given person is **male or female** based on three continuous features:\n","- **Height (feet)**\n","- **Weight (lbs)**\n","- **Foot Size (inches)**\n","\n","<img src=\"https://drive.google.com/uc?id=14hW9hH3rg0sZYGHnTFqGueiLzWjnkST-\" width=300/>\n","\n","| Gender  | Height (feet) | Weight (lbs) | Foot Size (inches) |\n","|---------|--------------|-------------|------------------|\n","| Male    | 6.00         | 180         | 12              |\n","| Male    | 5.92 (5'11\")| 190         | 11              |\n","| Male    | 5.58 (5'7\") | 170         | 12              |\n","| Male    | 5.92 (5'11\")| 165         | 10              |\n","| Female  | 5.00        | 100         | 6               |\n","| Female  | 5.50 (5'6\") | 150         | 8               |\n","| Female  | 5.42 (5'5\") | 130         | 7               |\n","| Female  | 5.75 (5'9\") | 150         | 9               |\n","\n","---\n","\n","## üîπ **Step 1: Compute Class Statistics**\n","To classify a new sample, we first compute the **mean and variance** of each feature for each class:\n","\n","| Gender  | Mean Height | Variance Height | Mean Weight | Variance Weight | Mean Foot Size | Variance Foot Size |\n","|---------|------------|----------------|-------------|----------------|---------------|----------------|\n","| Male    | 5.855      | 0.035          | 176.25      | 122.916        | 11.25         | 0.916         |\n","| Female  | 5.4175     | 0.097          | 132.5       | 558.333        | 7.5           | 1.6667        |\n","\n","We assume **equiprobable classes**, meaning:\n","\n","$$\n","P(\\text{Male}) = P(\\text{Female}) = 0.5\n","$$\n","\n","---\n","\n","## üîπ **Step 2: Classifying a New Sample**\n","A new person has the following features:\n","\n","| Height (feet) | Weight (lbs) | Foot Size (inches) |\n","|--------------|-------------|------------------|\n","| 6.00        | 130         | 8               |\n","\n","We now compute the **posterior probability** for each gender.\n","\n","---\n","\n","## üîπ **Step 3: Compute Likelihoods**\n","The likelihood of the given height, weight, and foot size under **each gender** is calculated using the **Gaussian probability density function**:\n","\n","For **Male:**\n","$$\n","P(\\text{Height} | \\text{Male}) = \\frac{1}{\\sqrt{2\\pi \\times 0.035}} \\exp \\left( -\\frac{(6.00 - 5.855)^2}{2 \\times 0.035} \\right)\n","$$\n","\n","For **Female:**\n","$$\n","P(\\text{Height} | \\text{Female}) = \\frac{1}{\\sqrt{2\\pi \\times 0.097}} \\exp \\left( -\\frac{(6.00 - 5.4175)^2}{2 \\times 0.097} \\right)\n","$$\n","\n","Similarly, we compute **$ P(\\text{Weight} | \\text{Gender}) $** and **$ P(\\text{Foot Size} | \\text{Gender}) $**.\n","\n","---\n","\n","## üîπ **Step 4: Compute Posterior Probability**\n","Since **$ P(\\text{today}) $** is the same for both classes, we only compare the numerators:\n","\n","For **Male:**\n","$$\n","P(\\text{Male} | \\text{Sample}) \\propto P(\\text{Male}) P(\\text{Height} | \\text{Male}) P(\\text{Weight} | \\text{Male}) P(\\text{Foot Size} | \\text{Male})\n","$$\n","\n","For **Female:**\n","$$\n","P(\\text{Female} | \\text{Sample}) \\propto P(\\text{Female}) P(\\text{Height} | \\text{Female}) P(\\text{Weight} | \\text{Female}) P(\\text{Foot Size} | \\text{Female})\n","$$\n","\n","From our calculations:\n","\n","\n","| Gender  | $ P(\\text{Height} | \\text{Gender}) $ | $ P(\\text{Weight} | \\text{Gender}) $ | $ P(\\text{Foot Size} | \\text{Gender}) $ | Posterior Numerator |\n","|---------|------------------------------|------------------------------|------------------------------|----------------------|\n","| Male    | 1.578                        | 5.9867e-06                   | 0.0013                       | $ 6.19707e-09 $      |\n","| Female  | 0.223                        | 0.0167                       | 0.2866                       | $ 0.00053 $          |\n","\n","\n","Since:\n","\n","$$\n","P(\\text{Female} | \\text{Sample}) > P(\\text{Male} | \\text{Sample})\n","$$\n","\n","we classify the **sample as Female**.\n","\n","---\n","\n","# üèÜ **Final Conclusion**\n","Using **Gaussian Na√Øve Bayes**, we determined that the given person is **Female** based on height, weight, and foot size.\n","\n","üöÄ **Next Step:** Implement this in Python using **Scikit-learn**!\n"],"metadata":{"id":"CJekXE7OjNwV"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n","\n","# Creating the dataset\n","data = {\n","    'Height': [6.00, 5.92, 5.58, 5.92, 5.00, 5.50, 5.42, 5.75],\n","    'Weight': [180, 190, 170, 165, 100, 150, 130, 150],\n","    'Foot_Size': [12, 11, 12, 10, 6, 8, 7, 9],\n","    'Gender': ['Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Female', 'Female']\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Encoding categorical variable (Gender: Male=1, Female=0)\n","df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n","\n","# Splitting features and target variable\n","X = df[['Height', 'Weight', 'Foot_Size']]\n","y = df['Gender']\n","\n","# Training the Gaussian Na√Øve Bayes model\n","model = GaussianNB()\n","model.fit(X, y)\n","\n","# New sample to classify\n","sample_data = pd.DataFrame([[6, 130, 8]], columns=['Height', 'Weight', 'Foot_Size'])  # Fixing feature names issue\n","\n","# Predicting the class\n","prediction = model.predict(sample_data)\n","predicted_class = 'Male' if prediction[0] == 1 else 'Female'\n","print(\"Predicted Gender:\", predicted_class)\n","\n","# Probability estimates for each class\n","probabilities = model.predict_proba(sample_data)\n","print(\"Probability (Male):\", probabilities[0][1])\n","print(\"Probability (Female):\", probabilities[0][0])\n","\n","# Predicting on training data\n","y_pred = model.predict(X)\n","\n","# Generating classification report\n","print(\"Classification Report:\\n\", classification_report(y, y_pred, target_names=['Female', 'male']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3qeEessL3tK","executionInfo":{"status":"ok","timestamp":1742146041144,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sujoy Sarkar Sujoy Sarkar","userId":"17509397499268237284"}},"outputId":"95e3f20b-a4e7-447e-c86a-5828b3892fb8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted Gender: Female\n","Probability (Male): 1.5442663163060025e-07\n","Probability (Female): 0.999999845573368\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","      Female       1.00      1.00      1.00         4\n","        male       1.00      1.00      1.00         4\n","\n","    accuracy                           1.00         8\n","   macro avg       1.00      1.00      1.00         8\n","weighted avg       1.00      1.00      1.00         8\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9mM80hIgvLie"},"execution_count":null,"outputs":[]}]}